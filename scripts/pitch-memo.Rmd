---
title: "Pitch Memo"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: united
    highlight: tango
    code_folding: hide
params:
  author: "Ben Jaques-Leslie"
  project_name: "Using fabricated data in analysis"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(skimr)
library(DataExplorer)
library(readr)
library(nycflights13)
library(arsenal)
library(patchwork)
library(estimatr)
library(gtsummary)
```

**Project name**: `r params$project_name`

**Author**: `r params$author`

# Narrative {.tabset .tabset-pills}

## Introduction 

Fabricated data can help to improve our research practices by reducing impulses by researchers to stray from analysis plans, requiring reproducible code that can take both fabricated and true data, and reduce the need for reanalysis. This memo discusses the difference between synthetic and fabricated data, how fabricated data may affect the need for reanalysis, when fabrication should occur, and provides a couple of examples of how to fabricate data.

### Synthetic and fabricated data

The most common reason for using data that mimics true micordata is concerns about confidentiality. In these cases, researchers may relay in synthetic. Researchers use synthetic data to perform analysis on data that contains personally identifiable information or cannot be shared. Synthetic data is created carefully to maintain the relationships between the variables. Analysis on synthetic data should produce the same results as if it were done on the true data. The `synthpop` package is a good example of a library that can created synthetic data.[^1]

[^1]: <https://cran.r-project.org/web/packages/synthpop/vignettes/synthpop.pdf>

Fabricated data has a different use than synthetic data. Fabrication is meant to strength our research by *removing* any correlation between variables, while maintain the distributions of the data elements. Even with a clearly defined analysis plan, a researcher will be tempted to dig into interesting relationships that they find during analysis, straying from their plan, and introducing research bias. Using fabricated data, a researcher knows that they are unlikely any relationships. If they do find something interesting, they know it is spurious. Researchers are more likely to adhere to their analysis plan. Furthermore, writing analysis scripts against fabricated data means the code will be tested with the true data as well, helping to assure that the code is reproducible.

### Fabrication and reanalysis

Reanalysis is conducted for several reasons:

1.  To check the check the original analysis code

2.  To review exploratory analysis

3.  To explore departures from the analysis plan.

Employing fabricated data mitigates the need for reanalysis in several ways. Writing analysis scripts against fabricated data implies a step of using that script against the true data. The process of using fabricated data includes a check of the code against another data set: the true data. Fabricated data pushes against exploratory analysis or departures from analysis plans because researchers are aware that any relationships they may see are not real and just randomly generate. Researchers should adhere more to analysis plans when writing scripts on fabricated data.

Using fabricated data reduces the necessity of reanalysis. The principle benefit of reanalysis when using fabricated data is that another researcher may have a different approach or conduct alternative tests on the data. Depending on the project one might believe that fabricated data replaces the need for any reanalysis.

### When to fabricate

Fabricated data is of greatest benefit at the analysis stage. During the analysis, researchers are evaluating the relationships between the data elements, exploring correlations, and identifying causation. Using fabricated data to write the scripts allows the researcher to do the work without bias created from seeing something interesting.

Data could be fabricated before data preparation, but it is a longer process and potentially more risky. Creating fabricated **before data preparation**, leads to the following process:

1.  Import original data set or sets

2.  Fabricated data set or sets[^2]

3.  Write data preparation script against fabricated data set

4.  Perform data preparation on fabricated data set to create fabricated analysis data

5.  Write analysis script based on analysis plan against the fabricated analysis data

6.  Perform data analysis on fabricated analysis data

7.  Perform data preparation on original data set to create analysis data

8.  Perform data analysis on analysis data

[^2]: Approach 1 is the better option here, because outliers and other data issues are more likely to be preserved. The data preparation will then need to be written to account for these issues.

The process for using fabricated data **after data preparation** is simpler:

1.  Import original data set or sets

2.  Write data preparation script against original data set to create analysis data

3.  Fabricate analysis data to create fabricated analysis data

4.  Write analysis script based on analysis plan against the fabricated analysis data

5.  Perform data analysis on fabricated analysis data

6.  Perform data analysis on analysis data

While using fabricated data at the data preparation stage would limit researcher degrees of freedom more, it is likely that data preparation will need to be adjusted when using the original data. One way this might happen is through removal of duplicates. Fabricated data removes correlations so the relationships that produce duplicated cases may be eliminated.[^3] Furthermore, adjustments in data preparation are likely to lead to changes in analysis. Using fabricated data at any stage is likely to require *some* changes when applying the true data, but this would be more limited if the data is fabricated at the analysis stage.

[^3]: For example, data reflecting cases at two points in time. The research is likely to want to select the data from the earlier or later point.

## Fabricating data {.tabset .tabset-pills}

### Original data

Below are two approaches to creating fabricated data. In the first approach, each column is independently shuffled. In the second, data is created by randomly drawing from the distributions of the true data.

In these examples, the `flights` data set will be used. The goal of fabrication is be remove correlations between data elements, while maintaining the distributions of the elements. For each approach, the fabricated data will be compared to the true data by summaries of the data (using `skimr::skim()`) and reviewing the correlations between the elements (using `DataExplorer::plot_correlation()`).

Below is a summary of the `flights` data set.

```{r}
flights %>% 
  skim()
```

Next are the correlations between the elements.

```{r include=FALSE}
og_corr_plot <- 
  flights %>% 
  plot_correlation(cor_args = list("use" = "pairwise.complete.obs")) +
  ggtitle("Original")
```

```{r}
og_corr_plot
```

### Approach 1: Shuffling the data by column

The first approach to creating fabricated data randomly shuffles each column is independently. This method has several benefits. First, it is very simple to perform. Each column is randomly sampled. Second, since the data is only shuffled, all outliers and data errors remain. This may be useful because these issues may have to be addressed in the analysis.

Below fabricated data is created using this approach.

```{r}
approach_01 <- 
  flights %>% 
  mutate(
    across(
      .cols = everything(),
      .fns = ~sample(.)
    )
  )
```

Below are summaries, comparing the fabricated data to the original data.

```{r}
flights %>% 
  mutate(data = "original") %>% 
  bind_rows(approach_01 %>% 
              mutate(data = "fabricated")) %>% 
  group_by(data) %>% 
  skim()
```

All of the summary statistics for each element and exactly the same between the fabricated data set and the original data set.

Next are the pairwise corrections in the fabricated data set compared to the original.

```{r,include=FALSE}
a1_corr_plot <- 
  approach_01%>% 
  plot_correlation(cor_args = list("use" = "pairwise.complete.obs")) +
  ggtitle("Approach 1")
```

```{r}
a1_corr_plot | og_corr_plot
```

Correlations between the data elements has changed greatly between original data the fabricated data using approach 1. Almost all pairwise correlations have been eliminated.

### Approach 2: Randomly drawing variables to mimic underlying distribution

The second approach creates a new data set where each column follows the same distribution as the data in the original data set. There are some benefits to this approach. First, this creates a data set with none of the true data. If data sharing is a constraint, this approach is more likely to be possible because the true data only acts to provide the structure, but none of it is used directly. Second, the data can be make with a larger number of observations. If a researcher is early in a study and wants to start developing analysis using the incomplete data, this approach may be used to create additional observations. This approach is more completed. Fabrication is data type dependent.

Below are functions used to fabricated different types of data.

#### Fabricating functions

The functions below were written with assistance from Joe Ritter at the University of Minnesota.

This function takes a categorical vector and generates a random distribution with the same probabilities.

```{r}
fabricateCat <- function(v, n) {
    # v is integer, string, or factor.
    # n is the desired length of the fabricate vector.
    w <- as.character(v)
    dist <- prop.table(table(w, useNA = 'always'))
    vals <- names(dist)
    fabricate.v <- sample(vals, n, prob=dist, replace=TRUE)
    if (is.factor(v)) fabricate.v <- factor(fabricate.v)
    if (is.integer(v)) fabricate.v <- as.integer(fabricate.v)
    return(fabricate.v)
}
```

This function takes a date vector, applies `fabricateCat()`, then converts the result back to a date type.

```{r}
fabricateDate <- function(v, n)
{
  lubridate::ymd(fabricateCat(v,n))
}
```

This function takes a continuous vector and generates a distribution with the same distribution.

```{r}
fabricateCont <- function(v, n) {
  # Remove nulls from v
     v <- na.omit(v)
 
    # v is a variable for which a kernel density estimate makes sense.
    # n is the desired length of the fabricate vector.
    dist <- density(v)
    # The simulation follows one of the examples for density().
    # The default kernel is gaussian, which is why rnorm is used.  
    # Bandwidth is the SD of the # kernel.
    fabricate.v <- rnorm(n, mean=sample(v, n, replace=TRUE), sd=dist$bw)
    return(fabricate.v)                    
}
```

This function applies the above based on the data type.

```{r}
fabricater <- function(in_original_data,in_variable, n)
{
  v <- 
    in_original_data %>% 
    select({{in_variable}}) %>% 
    pull({{in_variable}})
  
  data_type <- class(v)[1]
  
  # print(data_type)
  
  if(unique(v) %>% length() <= 3 & data_type != "Date")
  {
    data_type <- "character"
  }
  
  
  if(unique(v) %>% length() > 3)
  {
    data_type <- class(v)[1]
  }
  
  if(data_type == "Date")
  {
     out <- fabricateDate(
  v = v,
  n = n)
  }
  
  if(data_type == "factor")
  {
    out <-  fabricateCat(
  v = v,
  n = n)
  }
    
    if(data_type == "character")
  {
    out <-  fabricateCat(
  v = v,
  n = n)
    }
    
     if(data_type == "numeric")
  {
     out <- fabricateCont(
  v = v,
  n = n)
     }
  
    if(data_type == "integer")
  {
     out <- fabricateCat(
  v = v,
  n = n)
    }
  
  if(data_type == "POSIXct")
  {
     out <-
     as.POSIXct(
       fabricateCat(
         v = as.character(v),
         n = n)
       )
  }
  
  
  return(out)
}
```

#### Fabricating using approach 2

First, create list of columns to fabricate.

```{r}
column_to_fabricate <- 
  names(flights)

column_to_fabricate
```

Create dataframe of fabricated data based on the columns above and apply the names of those columns to the new dataframe. Note that the same number of rows as the original data set are being used in this example. The parameter `n` in the `fabricater()` function can be set to whatever size desired.

```{r include=FALSE}
approach_02 <- 
  column_to_fabricate %>% 
  map_dfc(~fabricater(flights,
                    all_of(.),
                    n = nrow(flights)
                    )
  ) %>% 
  rename_with(~column_to_fabricate)

```

Below are summaries, comparing the fabricated data to the original data.

```{r}
flights %>% 
  mutate(data = "original") %>% 
  bind_rows(approach_02 %>% 
              mutate(data = "fabricated")) %>% 
  group_by(data) %>% 
  skim()
```

With this approach, differences do exist between the data elements in the fabricated and original data sets, but they are very minimal.

Next are the pairwise corrections in the fabricated data set compared to the original.

```{r,include=FALSE}
a2_corr_plot <- 
  approach_02%>% 
  plot_correlation(cor_args = list("use" = "pairwise.complete.obs")) +
  ggtitle("Approach 2")
```

```{r}
a2_corr_plot | og_corr_plot
```

Again the correlations between the data elements has changed greatly. Almost all pairwise correlations have been eliminated.

### Analysis example

Say a researchers is interested in the association between arrival delays and departure delays accounting for the distance and the airport of origin. The relationship would be model with a linear model in the follow way:

$$
Y_i = \beta_0 + \beta_1DepartureDelay_i + \beta_2Distance_i + B_oOrigin_i + \epsilon
$$

The code below shows how the exact same script may be used on the original data and the data from each fabrication approach. The results differ for each.

```{r}
reg_sum <- function(in_data)
{
  in_data %>% 
    lm_robust(arr_delay ~ dep_delay + distance + origin, data = .) %>% 
  tbl_regression() %>% 
    add_significance_stars(
    pattern = "{estimate} ({conf.low}, {conf.high}){stars}",
    hide_ci = TRUE, hide_se = TRUE
  ) %>%
  modify_header(estimate ~ "**Beta (95% CI)**") %>%
  modify_footnote(estimate ~ "CI = Confidence Interval", abbreviation = TRUE)
}
```

```{r}
reg_flights <- 
  flights %>% 
  reg_sum()
reg_approach_01 <- 
  approach_01 %>% 
  reg_sum()
reg_approach_02 <- 
  approach_02 %>% 
  reg_sum()
```

```{r}
reg_merge <- 
  tbl_merge(
    tbls = list(reg_flights,reg_approach_01,reg_approach_02),
    tab_spanner = c("**True data**","**Approach 1**","**Approach 2**")
  )
reg_merge
```

## Appendix {.tabset .tabset-pills}

### Comparing data sets {.tabset .tabset-pills}

#### Approach 1 to original

```{r}
comp_1 <- comparedf(flights, approach_01)
```

```{r}
comp_1 %>%
  summary()
```

#### Approach 2 to original

```{r}
comp_2 <- comparedf(flights, approach_02)
```

```{r}
comp_2 %>%
  summary()
```
